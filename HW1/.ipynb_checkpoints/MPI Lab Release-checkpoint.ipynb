{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS205 Spring 2015: HW 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wesley Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Use Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VMWare Player (http://www.vmware.com/products/player) is the recommended software that I use, VirtualBox (https://www.virtualbox.org/wiki/Downloads) works too.\n",
    "\n",
    "Then download the Ubuntu 14.04 OS from http://www.ubuntu.com/download/desktop.\n",
    "\n",
    "Create a new virtual machine, all express settings are okay and install Ubuntu.\n",
    "\n",
    "Workflow: 1) Do all work in virtual machine or 2) Work outside and run inside just to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sudo apt-get install python.pip\n",
    "\n",
    "sudo apt-get install mpich\n",
    "# or\n",
    "sudo apt-get install openmpi-bin \n",
    "# (see http://stackoverflow.com/questions/2427399/mpich-vs-openmpi)\n",
    "\n",
    "sudo apt-get install python-dev\n",
    "sudo pip install mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Those Vehemently Against Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the best of my knowledge, mpi4py not supported on Windows though there happen to be local adaptations that work on the MS-MPI or other MPI standards\n",
    "\n",
    "Mac users can use brew/conda to install mpi4py and MPICH\n",
    "\n",
    "Will still require SSH (PuTTy, etc) to benchmark on clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Cluster Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have been given a username and temp password for your Research Computing account.\n",
    "\n",
    "The first step will be to log into change your password:\n",
    "\n",
    "A lot of the information can be found under the FAQ/documentation listed online: \n",
    "\n",
    "In general, you will need to SSH onto the node, and submit your jobs (shell script which we will try to provide a sample of if unfamiliar, but also detailed online on the Harvard Research Computing site).  Files will need to be transferred to and from the cluster with SCP or something.\n",
    "\n",
    "More information will be posted on Piazza and we won't spend time setting this up in lab today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mechanics of MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "# sets up communicator object\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "# gets rank of the processors inside thie communicator\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# gets number of processors in the communicator\n",
    "size = comm.Get_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point to Point Communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "send, recv, isend, (no irecv), sendrecv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collective Communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bcast, reduce, scatter, gather, allreduce, allgather, alltoall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Capitalize or Not to Capitalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI_send() vs MPI_Send() etc\n",
    "\n",
    "All-lowercase methods are used for communication of generic Python objects\n",
    "\n",
    "Note: under the hood, this is down with Pickle (https://docs.python.org/2/library/pickle.html)\n",
    "\n",
    "Uppercase letter methods are used for buffer-provided objects (like NumPy arrays!) and requires arguments with more detail like [data, count, MPI.DOUBLE] where count things of MPI.DOUBLE type size are read form the given buffer called data (http://mpi4py.scipy.org/docs/usrman/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpirun -n 4 python myCode.py\n",
    "mpiexec -n 4 python myCode.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mpirun vs mpiexec? http://forthescience.org/blog/2013/02/15/difference-between-mpiexec-and-mpirun/\n",
    "\n",
    "Mostly historical - mpiexec is the modern command, mpirun is the backwards compatible one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing in Serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time module: time.time()\n",
    "\n",
    "differences give start and end times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comm.barrier() - to sync up\n",
    "\n",
    "MPI.Wtime() - just like time.time(), save differences in times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a sample MPI program where each processor simply reports their rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    comm = MPI.COMM_WORLD\n",
    "\n",
    "    # gets rank of the processors inside this communicator\n",
    "    rank = comm.Get_rank()\n",
    "    print rank\n",
    "    \n",
    "    #gets number of processers in this communicator\n",
    "    size = comm.Get_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think in parallel.  We wish to use a Monte Carlo method to compute the value of $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sample points in side the unit square.  Find the ratio of the points that also fall in the unit circle and multiple this number by 4.\n",
    "\n",
    "$\\rho = \\frac{A_{\\textrm{circle}}}{A_{\\textrm{Square}}} = \\frac{\\pi r^2}{(2r)^2} = \\frac{\\pi}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf: 3.1422\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def mc_pi(n, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    count = 0.0\n",
    "    for i in xrange(n):\n",
    "        testPt = np.random.uniform(-1, 1, size=2)\n",
    "        if np.linalg.norm(testPt) < 1:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = 100000\n",
    "    pi_est = mc_pi(n)/n * 4.0\n",
    "    print \"asdf: \" + str(pi_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "from mc_pi import mc_pi\n",
    "\n",
    "def parallel_mc_pi(n, comm, p_root=0):\n",
    "    rank=comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    \n",
    "    myCount = mc_pi(n/size, seed=rank)\n",
    "    \n",
    "    print \"rank: %d, myCount: %d\" % (rank, myCount)\n",
    "    \n",
    "    totalCount = comm.reduce(myCount, op = MPI.SUM, root=p_root)\n",
    "    return totalCount\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    \n",
    "    # use numPoints points in MC\n",
    "    \n",
    "    numPoints = 1000000\n",
    "    comm.barrier()\n",
    "    p_start = MPI.Wtime()\n",
    "    \n",
    "    p_answer = parallel_mc_pi(numPoints, comm)\n",
    "    comm.barrier()\n",
    "    p_stop = MPI.Wtime()\n",
    "    \n",
    "    if rank == 0:\n",
    "        p_answer = (4*p_answer) / numPoints\n",
    "        \n",
    "    if rank == 0:\n",
    "        s_start = time.time()\n",
    "        s_answer = (4.0 * mc_pi(numPoints) / numPoints )\n",
    "        s_stop = time.time()\n",
    "\n",
    "        print \"Serial Time: %0.6f secs\" % s_stop\n",
    "        print \"Parallel Time: %0.6f secs\" % p_stop\n",
    "        print \"Serial Result = %0.6f\" % s_answer\n",
    "        print \"Parallel Result = %0.6f\" % p_answer\n",
    "        print \"NumPy = %d\" % np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Product Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two random large arrays and evaluate the inner product of the two vectors.  The goal is to compute first in serial, then in parallel.  You can make simplifying assumptions as necessary for this lab.\n",
    "\n",
    "The inner product formula is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{a \\cdot b} = \\sum\\limits_{k=0}^{K-1}a_k b_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a*b = 2498917.903177 in 9.774429 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def get_big_arrays():\n",
    "  '''Generate two big random arrays.'''\n",
    "  N = 10000000      # A big number, the size of the arrays.\n",
    "  np.random.seed(0)  # Set the random seed\n",
    "  return np.random.random(N), np.random.random(N)\n",
    "\n",
    "def serial_dot(a, b):\n",
    "  '''The dot-product of the arrays -- slow implementation using for-loop.'''\n",
    "  result = 0\n",
    "  for k in xrange(0, len(a)):\n",
    "    result += a[k]*b[k]\n",
    "  return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Get big arrays\n",
    "  a, b = get_big_arrays()\n",
    "\n",
    "  # Compute the dot product in serial\n",
    "  start_time = time.time()\n",
    "  result = serial_dot(a, b)\n",
    "  end_time = time.time()\n",
    "\n",
    "  print \"a*b = %f in %f seconds\" % (result, end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from P1serial import get_big_arrays, serial_dot\n",
    "\n",
    "def parallel_dot(a, b, comm, p_root=0):\n",
    "  '''The parallel dot-product of the arrays a and b.\n",
    "  Assumes the arrays exist on process p_root and returns the result to\n",
    "  process p_root.\n",
    "  By default, p_root = process 0.'''\n",
    "\n",
    "  # TODO\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  comm = MPI.COMM_WORLD\n",
    "  rank = comm.Get_rank()\n",
    "\n",
    "  # Get big arrays on process 0\n",
    "  a, b = None, None\n",
    "  if rank == 0:\n",
    "    a, b = get_big_arrays()\n",
    "\n",
    "  # TODO"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
